{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "run_id = 1\n",
    "main_folder_path = pathlib.Path(\"/home/alp/noetic_ws/src/simulation/images\")\n",
    "folder_path = main_folder_path / f\"run_{run_id}\"\n",
    "csv_path = folder_path / \"cargo_data.csv\"\n",
    "copy_similar = True  # Changed from delete_similar to copy_similar\n",
    "\n",
    "# Create output folder for non-recurrent frames\n",
    "output_folder = main_folder_path / \"non_recurrent\"\n",
    "output_csv_path = main_folder_path / \"cargo_data.csv\"\n",
    "\n",
    "def normalize(q):\n",
    "    return q / np.linalg.norm(q)\n",
    "\n",
    "def quat_angle_diff(q1, q2):\n",
    "    dot = np.clip(np.dot(q1, q2), -1.0, 1.0)\n",
    "    angle_rad = 2 * np.arccos(abs(dot))\n",
    "    return np.degrees(angle_rad)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Normalize quaternions\n",
    "quats = df[['cargo_rot_x', 'cargo_rot_y', 'cargo_rot_z', 'cargo_rot_w']].apply(lambda row: normalize(row.values), axis=1)\n",
    "\n",
    "# Reference quaternion (identity quaternion)\n",
    "ref_q = normalize(np.array([0.0, 0.0, 0.0, 1.0]))\n",
    "\n",
    "# Thresholds\n",
    "abs_threshold = 5.0\n",
    "seq_threshold = 2.0\n",
    "\n",
    "# Counters\n",
    "abs_close_count = 0\n",
    "seq_close_count = 0\n",
    "\n",
    "# Lists to track what to copy (keep the diverse/representative frames)\n",
    "files_to_copy = []\n",
    "frame_ids_to_keep = set()\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    q = quats[i]\n",
    "    frame_id = int(df.loc[i, 'frameId'])\n",
    "    \n",
    "    # Check if close to reference quaternion\n",
    "    angle_from_ref = quat_angle_diff(ref_q, q)\n",
    "    \n",
    "    if angle_from_ref < abs_threshold:\n",
    "        abs_close_count += 1\n",
    "        # Skip this frame entirely (too close to reference)\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    # This frame is good - add it to keep list\n",
    "    fname = f\"{frame_id}.jpg\"\n",
    "    files_to_copy.append(fname)\n",
    "    frame_ids_to_keep.add(frame_id)\n",
    "    \n",
    "    # Now check how many subsequent frames are similar to THIS frame (not to each other)\n",
    "    base_q = q  # This is our reference frame for the sequence\n",
    "    j = i + 1\n",
    "    \n",
    "    while j < len(df):\n",
    "        next_q = quats[j]\n",
    "        angle_from_base = quat_angle_diff(base_q, next_q)  # Compare to the base frame, not previous\n",
    "        \n",
    "        if angle_from_base < seq_threshold:\n",
    "            seq_close_count += 1\n",
    "            j += 1  # Skip this similar frame\n",
    "        else:\n",
    "            break  # Found a different frame, stop the sequence\n",
    "    \n",
    "    # Move index to the next different frame\n",
    "    i = j\n",
    "\n",
    "if copy_similar:\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy files to the new folder\n",
    "    copied_count = 0\n",
    "    for file_name in files_to_copy:\n",
    "        source_path = folder_path / file_name\n",
    "        dest_path = output_folder / file_name\n",
    "        \n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_count += 1\n",
    "        else:\n",
    "            print(f\"Warning: {file_name} not found in source folder\")\n",
    "    \n",
    "    # Create new CSV with only the frames we're keeping (diverse/representative ones)\n",
    "    # Filter dataframe to keep only the frames we identified as diverse\n",
    "    filtered_df = df[df['frameId'].isin(frame_ids_to_keep)]\n",
    "    \n",
    "    # Sort by frameId to maintain sequentiality\n",
    "    filtered_df = filtered_df.sort_values('frameId').reset_index(drop=True)\n",
    "    \n",
    "    # Save the filtered CSV\n",
    "    filtered_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Created output folder: {output_folder}\")\n",
    "    print(f\"Copied {copied_count} image files\")\n",
    "    print(f\"Created new CSV with {len(filtered_df)} rows\")\n",
    "\n",
    "print(f\"Original total frames: {len(df)}\")\n",
    "print(f\"Total close-to-zero frames: {abs_close_count}\")\n",
    "print(f\"Total sequentially close frames: {seq_close_count}\")\n",
    "print(f\"Total frames to copy (diverse representatives): {len(files_to_copy)}\")\n",
    "\n",
    "if copy_similar:\n",
    "    print(f\"Diverse/representative frames saved to: {output_folder}\")\n",
    "    print(f\"New CSV contains {len(filtered_df)} frames (maintaining sequential order)\")\n",
    "else:\n",
    "    print(f\"Would copy {len(files_to_copy)} diverse frames to separate folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from model import StateBasedModel\n",
    "from dataloader import StateBasedDroneDataset\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "CONFIG = {\n",
    "    \"base_folder\": \"/home/alp/noetic_ws/src/simulation/images/run_8\",\n",
    "    \"sequence_length\": 10,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"state_dim\": 128,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"λ_rot\": 0.8,\n",
    "    \"λ_dir\": 1.5,\n",
    "    \"λ_dist\": 0.1,\n",
    "    \"λ_pos\": 1.5,\n",
    "    \"λ_vel\": 0.2,\n",
    "    \"λ_temporal\": 0.3,\n",
    "    \"λ_state_reg\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"gradient_clip\": 1.0,\n",
    "    \"val_check_interval\": 0.5,\n",
    "    \"early_stop_patience\": 15,\n",
    "}\n",
    "\n",
    "def create_datasets():\n",
    "    dataset = StateBasedDroneDataset(\n",
    "        images_folder=CONFIG[\"base_folder\"],\n",
    "        csv_path=os.path.join(CONFIG[\"base_folder\"], \"cargo_data.csv\"),\n",
    "        sequence_length=CONFIG[\"sequence_length\"],\n",
    "        augment=False,\n",
    "        start_from_zero_state=True\n",
    "    )\n",
    "    \n",
    "    total_sequences = len(dataset)\n",
    "    n_train_seq = int(0.7 * total_sequences)\n",
    "    n_val_seq = int(0.15 * total_sequences)\n",
    "    \n",
    "    train_indices = list(range(0, n_train_seq))\n",
    "    val_indices = list(range(n_train_seq, n_train_seq + n_val_seq))\n",
    "    test_indices = list(range(n_train_seq + n_val_seq, total_sequences))\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    dataset.augment = True\n",
    "    \n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=True, drop_last=True, persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def create_model():\n",
    "    return StateBasedModel(\n",
    "        λ_rot=CONFIG[\"λ_rot\"], λ_dir=CONFIG[\"λ_dir\"], λ_dist=CONFIG[\"λ_dist\"],\n",
    "        λ_pos=CONFIG[\"λ_pos\"], λ_vel=CONFIG[\"λ_vel\"], λ_temporal=CONFIG[\"λ_temporal\"],\n",
    "        λ_state_reg=CONFIG[\"λ_state_reg\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        state_dim=CONFIG[\"state_dim\"], dropout_rate=CONFIG[\"dropout_rate\"]\n",
    "    )\n",
    "\n",
    "def setup_trainer():\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_loss', mode='min', save_top_k=3,\n",
    "            filename='state_model-{epoch:02d}-{val_loss:.4f}',\n",
    "            dirpath='./checkpoints/state_based_model', save_last=True\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_loss', patience=CONFIG[\"early_stop_patience\"], mode='min', verbose=True),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "    \n",
    "    logger = TensorBoardLogger(save_dir='lightning_logs', name='state_based_cargo_tracking', version=None)\n",
    "    \n",
    "    return pl.Trainer(\n",
    "        logger=logger, callbacks=callbacks, max_epochs=CONFIG[\"num_epochs\"],\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu', devices=1,\n",
    "        precision='16-mixed' if torch.cuda.is_available() else '32',\n",
    "        gradient_clip_val=CONFIG[\"gradient_clip\"], val_check_interval=CONFIG[\"val_check_interval\"],\n",
    "        log_every_n_steps=10, accumulate_grad_batches=2, deterministic=True\n",
    "    )\n",
    "\n",
    "def calculate_errors(pred_numpy, label_numpy):\n",
    "    \"\"\"Calculate comprehensive errors\"\"\"\n",
    "    pred_dir = pred_numpy[:3]\n",
    "    pred_dist = pred_numpy[3]\n",
    "    pred_rot = pred_numpy[4:8]\n",
    "    pred_vel = pred_numpy[8:11]\n",
    "    \n",
    "    gt_dir = label_numpy[:3]\n",
    "    gt_dist = label_numpy[3]\n",
    "    gt_rot = label_numpy[4:8]\n",
    "    gt_vel = label_numpy[8:11]\n",
    "    \n",
    "    # Direction error\n",
    "    pred_dir_norm = pred_dir / (np.linalg.norm(pred_dir) + 1e-8)\n",
    "    gt_dir_norm = gt_dir / (np.linalg.norm(gt_dir) + 1e-8)\n",
    "    cos_sim = np.clip(np.dot(pred_dir_norm, gt_dir_norm), -1.0, 1.0)\n",
    "    direction_error = np.degrees(np.arccos(np.abs(cos_sim)))\n",
    "    \n",
    "    # Distance errors\n",
    "    distance_error_abs = abs(pred_dist - gt_dist)\n",
    "    distance_error_rel = distance_error_abs / max(abs(gt_dist), 1e-6) * 100\n",
    "    \n",
    "    # Rotation error\n",
    "    pred_rot_norm = pred_rot / (np.linalg.norm(pred_rot) + 1e-8)\n",
    "    gt_rot_norm = gt_rot / (np.linalg.norm(gt_rot) + 1e-8)\n",
    "    dot_product = np.abs(np.dot(pred_rot_norm, gt_rot_norm))\n",
    "    rotation_error = 2 * np.degrees(np.arccos(np.clip(dot_product, 0, 1)))\n",
    "    \n",
    "    # Velocity error\n",
    "    velocity_error = np.linalg.norm(pred_vel - gt_vel)\n",
    "    \n",
    "    # Position error\n",
    "    pred_pos = pred_dir_norm * pred_dist\n",
    "    gt_pos = gt_dir_norm * gt_dist\n",
    "    position_error = np.linalg.norm(pred_pos - gt_pos)\n",
    "    \n",
    "    return {\n",
    "        'direction_error_degrees': direction_error,\n",
    "        'distance_error_abs': distance_error_abs,\n",
    "        'distance_error_rel': distance_error_rel,\n",
    "        'rotation_error_degrees': rotation_error,\n",
    "        'velocity_error_magnitude': velocity_error,\n",
    "        'position_error_magnitude': position_error\n",
    "    }\n",
    "\n",
    "def analyze_test_set(model, test_loader):\n",
    "    \"\"\"Analyze test set predictions over time and save to CSV\"\"\"\n",
    "    print(\"Analyzing test set predictions...\")\n",
    "    \n",
    "    # Create CSV file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"test_predictions_{timestamp}.csv\"\n",
    "    \n",
    "    headers = [\n",
    "        'batch_idx', 'sample_idx', 'frame_idx', 'sequence_time',\n",
    "        'pred_dir_x', 'pred_dir_y', 'pred_dir_z', 'gt_dir_x', 'gt_dir_y', 'gt_dir_z',\n",
    "        'pred_distance', 'gt_distance',\n",
    "        'pred_rot_x', 'pred_rot_y', 'pred_rot_z', 'pred_rot_w',\n",
    "        'gt_rot_x', 'gt_rot_y', 'gt_rot_z', 'gt_rot_w',\n",
    "        'pred_vel_x', 'pred_vel_y', 'pred_vel_z', 'gt_vel_x', 'gt_vel_y', 'gt_vel_z',\n",
    "        'direction_error_degrees', 'distance_error_abs', 'distance_error_rel',\n",
    "        'rotation_error_degrees', 'velocity_error_magnitude', 'position_error_magnitude'\n",
    "    ]\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        model.eval()\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_loader):\n",
    "                images = batch['image_sequence'].to(model.device)\n",
    "                labels = batch['label_sequence'].to(model.device)\n",
    "                \n",
    "                # Get predictions for entire sequence\n",
    "                predictions, _, _ = model(images, return_all_states=True)\n",
    "                \n",
    "                # Process each sample and frame in the batch\n",
    "                batch_size, seq_len = images.shape[0], images.shape[1]\n",
    "                \n",
    "                for sample_idx in range(batch_size):\n",
    "                    for frame_idx in range(seq_len):\n",
    "                        pred = predictions[sample_idx, frame_idx].cpu().numpy()\n",
    "                        gt = labels[sample_idx, frame_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Calculate errors\n",
    "                        errors = calculate_errors(pred, gt)\n",
    "                        \n",
    "                        # Prepare CSV row\n",
    "                        csv_row = [\n",
    "                            batch_idx, sample_idx, frame_idx, frame_idx * 0.1,  # assuming 10fps\n",
    "                            # Predictions and ground truth\n",
    "                            pred[0], pred[1], pred[2], gt[0], gt[1], gt[2],  # direction\n",
    "                            pred[3], gt[3],  # distance\n",
    "                            pred[4], pred[5], pred[6], pred[7], gt[4], gt[5], gt[6], gt[7],  # rotation\n",
    "                            pred[8], pred[9], pred[10], gt[8], gt[9], gt[10],  # velocity\n",
    "                            # Errors\n",
    "                            errors['direction_error_degrees'], errors['distance_error_abs'],\n",
    "                            errors['distance_error_rel'], errors['rotation_error_degrees'],\n",
    "                            errors['velocity_error_magnitude'], errors['position_error_magnitude']\n",
    "                        ]\n",
    "                        \n",
    "                        writer.writerow(csv_row)\n",
    "                        total_samples += 1\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Processed batch {batch_idx}/{len(test_loader)}\")\n",
    "    \n",
    "    print(f\"Test analysis complete. Saved {total_samples} predictions to {csv_filename}\")\n",
    "    return csv_filename\n",
    "\n",
    "def plot_test_analysis(csv_filename):\n",
    "    \"\"\"Generate time series plots for test predictions\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        print(f\"Loaded {len(df)} test predictions from {csv_filename}\")\n",
    "        \n",
    "        # Create time index (continuous time across all sequences)\n",
    "        df['global_time'] = df['batch_idx'] * CONFIG[\"sequence_length\"] * 0.1 + df['sequence_time']\n",
    "        \n",
    "        # Create plots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        error_configs = [\n",
    "            ('direction_error_degrees', 'Direction Error (°)', 'blue'),\n",
    "            ('distance_error_abs', 'Distance Error (m)', 'red'),\n",
    "            ('distance_error_rel', 'Distance Error (%)', 'orange'),\n",
    "            ('rotation_error_degrees', 'Rotation Error (°)', 'green'),\n",
    "            ('velocity_error_magnitude', 'Velocity Error (m/s)', 'purple'),\n",
    "            ('position_error_magnitude', 'Position Error (m)', 'brown')\n",
    "        ]\n",
    "        \n",
    "        for idx, (col, title, color) in enumerate(error_configs):\n",
    "            ax = axes[idx]\n",
    "            times = df['global_time'].values\n",
    "            errors = df[col].values\n",
    "            \n",
    "            # Plot with transparency to show density\n",
    "            ax.plot(times, errors, color=color, linewidth=0.5, alpha=0.6)\n",
    "            \n",
    "            # Add rolling average\n",
    "            if len(errors) > 100:\n",
    "                window = min(100, len(errors) // 10)\n",
    "                rolling_mean = pd.Series(errors).rolling(window=window, center=True).mean()\n",
    "                ax.plot(times, rolling_mean, color='darkred', linewidth=2, label=f'Rolling Mean (w={window})')\n",
    "            \n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Time (seconds)')\n",
    "            ax.set_ylabel('Error')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_error = np.mean(errors)\n",
    "            ax.axhline(y=mean_error, color='red', linestyle='--', alpha=0.7, \n",
    "                      label=f'Mean: {mean_error:.3f}')\n",
    "            if len(errors) > 100:\n",
    "                ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = csv_filename.replace('.csv', '_plots.png')\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Test analysis plots saved to: {plot_filename}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"TEST SET ANALYSIS SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total predictions: {len(df)}\")\n",
    "        print(f\"Total time span: {df['global_time'].max():.1f} seconds\")\n",
    "        print(f\"Unique sequences: {df['batch_idx'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n{'Metric':<25} {'Mean':<10} {'Std':<10} {'Max':<10}\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        for col, title, _ in error_configs:\n",
    "            values = df[col].values\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            max_val = np.max(values)\n",
    "            print(f\"{title:<25} {mean_val:<10.3f} {std_val:<10.3f} {max_val:<10.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in test analysis: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets()\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "    \n",
    "    # Create model and trainer\n",
    "    model = create_model()\n",
    "    trainer = setup_trainer()\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Test\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path='best')\n",
    "    \n",
    "    for metric, value in test_results[0].items():\n",
    "        if 'loss' in metric:\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        elif 'degrees' in metric:\n",
    "            print(f\"  {metric}: {value:.2f}°\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Load best model for detailed test analysis\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    model = StateBasedModel.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Analyze test set predictions over time\n",
    "    csv_filename = analyze_test_set(model, test_loader)\n",
    "    plot_test_analysis(csv_filename)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"state_based_weights.pth\")\n",
    "    trainer.save_checkpoint(\"state_based_model_final.ckpt\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from dataloader import DroneDataset\n",
    "from model import Model\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "SEQUENCE_LENGTH = 5  # Number of consecutive frames to use\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 8\n",
    "\n",
    " = 1e-4\n",
    "FPS = 10  # Your video frame rate\n",
    "\n",
    "full_dataset = DroneDataset(\n",
    "    images_folder=\"./data/images\",\n",
    "    csv_path=\"./data/cargo_data.csv\",\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_end = int(0.7 * total_size)\n",
    "val_end = train_end + int(0.15 * total_size)\n",
    "test_indices = list(range(val_end, total_size))\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,  # Using full dataset for testing like in original\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = Model(\n",
    "    λ_rot=0.8,    # Rotation loss weight\n",
    "    λ_dir=0.5,    # Direction vector loss weight\n",
    "    λ_dist=0.1,   # Distance loss weight\n",
    "    λ_pos=1.5,    # Position loss weight\n",
    "    λ_vel=0.3,    # Velocity loss weight\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "# Load weights from checkpoint file\n",
    "checkpoint_path = \"weights.ckpt\"\n",
    "model = Model.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "# Test the model using the loaded weights\n",
    "trainer.test(model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
